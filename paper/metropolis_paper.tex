\documentclass{article}

\usepackage[a4paper,margin=1.15in,footskip=0.25in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textpos}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{varwidth}
\usepackage[linesnumbered,ruled]{algorithm2e}

% Theorem
\newtheorem{theorem}{Theorem}

% lists
\usepackage{outlines}
\usepackage{enumitem}
\newenvironment{tight_enum}{
\begin{enumerate}[label=\alph*.]
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
}{\end{enumerate}}

% \subsubsubsection{}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\newcommand\simpleparagraph[1]{%
  \stepcounter{paragraph}\paragraph*{\theparagraph\quad{}#1}}


%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%

\begin{document}

\title{An optimization-inspired approach to parallel sorting}
\author{Team Metropolis: \\
		James Farzi, JJ Lay, and Graham West}
\date{COMS 7900, Capstone}

\maketitle

\begin{abstract}
We present a novel method influenced by ideas in the field optimization to efficiently sort large amounts of data in parallel on a cluster of computing nodes. We describe in depth the different challenges which beset such a method, including distributing/importing files, locally sorting the data on each node, uniformly binning the data, and exchanging data between the nodes. We also present the results of several different timing tests applied to the method. These tests demonstrate how the method scales when the number of files and/or the number of nodes is increased. Finally, we summarize the the greatest challenges in implementing the method, as well as the components of the method which were the most successful. We conclude with a discussion on ways in which the method could be improved.
\end{abstract}


\tableofcontents


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{The Method}

Introduction:

We used C++ with C MPI calls

Used GitHub

Workflow description\\



Conventions (NOTE: be consistent with the indices):

HEAD node (not master node)

$N$: number of nodes

$W$: number of workers ($N-1$)

$L$: number of lines to read per file

$L_w$: number of lines on the $w$th worker

$M$: max number of allowed time steps

Indices:
\begin{tight_enum}
	\item $m = 0, \cdots, M$ is the time step of the bin adaptation scheme (likely less than $M$)
	\item $n = 0, \cdots, W$ spans the nodes
	\item $w = 1, \cdots, W$ spans the workers
	\item $i = 0, \cdots, W$ spans the bin edges/indices
	\item $j = 1, \cdots, W$ spans the bin counts (this will occasionally subscript binI/E as well)
	\item $\ell_w = 0, \cdots, L_w-1$ spans the lines on the $w$th worker
	\item $k = 0, \cdots, 3$ is the data column being sorted
\end{tight_enum}

$\textrm{data}^w_{4\ell+k}$: data point on $k$th line and $\ell$th column (0 indexed) on the $w$th worker (1 indexed)

$\textrm{binE}^m_j$: bin edges (0 indexed) at time step $m$

$\textrm{binI}^{w,m}_j$: bin indices on worker $w$ (0 indexed)

$\textrm{binC}^{w,m}_j$: bin counts on worker $w$ (1 indexed w.r.t. $w$) at time step $m$

$\textrm{binC}^{0,m}_j$: bin counts on head node (sum of worker binC's) at time step $m$



% JJ
\subsection{File I/O}
\subsubsection{Distributing files}
\subsubsection{Importing files}


% JF
\subsection{Sorting}
\subsubsection{Linked list merge sort}
\subsubsection{Bubble sort}


% GW
\subsection{Binning}
% This is where the optimization stuff is
We now discuss the optimization-inspired portion of the method. It has optimization properties since we need to find the bin edges such that we maximum the uniformity of the distribution. However, it is a constrained form of optimization since the bin edges must always maintain the relation
\begin{equation}
	\textrm{binE}^m_i < \textrm{binE}^m_{i+1}
\end{equation}
As we will see, the way we construct our adaptation formulae ensures that this constraint is always met.

\subsubsection{Data binning w/ binary search}
Since we use an iterative scheme to adjust the bin edges through time, we use as an initial condition/approximation equally-spaced bin edges between the global min and max of the data. Now, since each worker could theoretically have millions of data points, repeatedly looping through the data to find the bin in which each point lies would be inefficient. As such, we use the fact that the data is sorted to expedite the process significantly by using binary search. Now, each worker will have the same bin edges $\textrm{binE}^m_i$ (where $i = 0, \cdots, W$), but the the location of the edges with respect to the data indices on each worker will likely be quite different if the data distribution across workers varies. With this in mind, for each worker, we search for the data indices at which each bin edge lies, giving us the new variables $\textrm{binI}^{w,m}_i$ (note the superscript $w$, indicating that each worker has its own unique set of bin indices). To do this, we search for the index $\ell$ such that
\begin{equation}
	\textrm{data}^w_{4\ell+k} < \textrm{binE}^m_i < \textrm{data}^w_{{4(\ell+1)+k}}
\end{equation}
giving $\textrm{binI}^{w,m}_i = \ell+1$. We also set $\textrm{binI}^{w,m}_0 = 0$ and $\textrm{binI}^{w,m}_W = L_w$. We then calculate $\textrm{binC}^{w,m}_j$:
\begin{equation}
	\textrm{binC}^{w,m}_j = \textrm{binI}^{w,m}_j - \textrm{binI}^{w,m}_{j-1}, \quad j = 1, \cdots, W
\end{equation}
Lastly, all workers send their bin counts to the head node and we calculate the total bin counts:
\begin{equation}
	\textrm{binC}^{0,m}_j = \sum_{w=1}^{W} \textrm{binC}^{w,m}_j, \quad j = 1, \cdots, W
\end{equation}


\subsubsection{Stopping criterion: the uniformity metric}
Once the head node has calculated the total bin counts it then determines how uniformly the data distributed across the bins:
\begin{equation}
	U^m = \textrm{max} \bigg( \dfrac{\textrm{binC}^{0,m}_{\textrm{max}} - \textrm{binC}^{0,m}_{\textrm{avg}}}{\textrm{binC}^{0,m}_{\textrm{avg}}}, \dfrac{\textrm{binC}^{0,m}_{\textrm{avg}} - \textrm{binC}^{0,m}_{\textrm{min}}}{\textrm{binC}^{0,m}_{\textrm{avg}}} \bigg)
\end{equation}
If $U$ is below a set threshold (usually $\approx 0.1$), then the the data distribution is deemed to be uniform in the sense that each worker will have within $\approx 10\%$ of the average data per worker; thus sorting on each worker will take roughly equal time.


\subsubsection{Adapting bin edges}
If the data is not uniform in the first step, we move on to adapt the interior bin edges:	\begin{equation}
	\begin{split}
		\Delta C & = 2.0 ( \textrm{binC}^{0,m}_i - \textrm{binC}^{0,m}_{i-1} ) / ( \textrm{binC}^{0,m}_i + \textrm{binC}^{0,m}_{i-1} ) \\
		\Delta B & = \textrm{binE}^m_{i+1} - \textrm{binE}^m_i \\
		\textrm{binE}^{m+1}_i & = \textrm{binE}^m_i + \alpha \Delta C \Delta B
	\end{split}
\end{equation}
where $0 < \alpha < 0.5$. Now, each of these terms is designed the allow the bins to adapt the maximum amount possible without the bin edges becoming out of order.







% JF
\subsection{Exchanging data}
\subsubsection{Data swap}
\subsubsection{Cleanup}


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Testing}


\subsection{File I/O}

\subsection{Sorting}
\subsubsection{Linked list merge sort}
\subsubsection{Bubble sort}

\subsection{Binning}
\subsubsection{Prototyping in MATLAB}
\subsubsection{C++ runs}

\subsection{Exchanging data}



%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Conclusions}
We will now conclude with two discussions on 1) the most difficult and most successful aspects of our method and 2) ways of improving the both the method's performance/efficiency and our workflow as a group.

\subsection{Challenges and successes}


\subsection{Future work}



%%%%%%%%%%%%%%%
%%%%% THE END %%%%%
%%%%%%%%%%%%%%%


\end{document}





