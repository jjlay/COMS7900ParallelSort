\documentclass{article}

\usepackage[a4paper,margin=1.15in,footskip=0.25in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textpos}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{varwidth}
\usepackage[linesnumbered,ruled]{algorithm2e}

% Theorem
\newtheorem{theorem}{Theorem}

% lists
\usepackage{outlines}
\usepackage{enumitem}
\newenvironment{tight_enum}{
\begin{enumerate}[label=\alph*.]
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
}{\end{enumerate}}

% \subsubsubsection{}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\newcommand\simpleparagraph[1]{%
  \stepcounter{paragraph}\paragraph*{\theparagraph\quad{}#1}}

\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{mdframed}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% \usepackage{courier}
\lstset{%frame=tb,
language=C++,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
basicstyle={\small\ttfamily},
numbers=none,
numberstyle=\tiny\color{gray},
keywordstyle=\color{blue},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%

\begin{document}

\title{An optimization-inspired approach to parallel sorting}
\author{Team Metropolis: \\
		James Farzi, JJ Lay, and Graham West}
\date{COMS 7900, Capstone}

\maketitle

\begin{abstract}
We present a novel method influenced by ideas in the field optimization to efficiently sort large amounts of data in parallel on a cluster of computing nodes. We describe in depth the different challenges which beset such a method, including distributing/importing files, locally sorting the data on each node, uniformly binning the data, and exchanging data between the nodes. We also present the results of several different timing tests applied to the method. These tests demonstrate how the method scales when the number of files and/or the number of nodes is increased. Finally, we summarize the the greatest challenges in implementing the method, as well as the components of the method which were the most successful. We conclude with a discussion on ways in which the method could be improved.
\end{abstract}


\tableofcontents


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{The Method}

Introduction:



\subsection{Overview}


\subsubsection{Workflow}
We used C++ with C MPI calls

Used GitHub

Workflow description


\subsubsection{Variables and conventions}
%%% NOTE %%%
%%% NOTE %%%
% 1) be consistent with the indices:
% 2) say head node (not master node)
% 3) say worker (not node) whenever possible
%%% NOTE %%%
%%% NOTE %%%

Here we provide a helpful list of conventions, notations, and variable names used throughout this paper.

\begin{mdframed}[backgroundcolor=blue!20]
	Counts:
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{itemize}
		\setlength\itemsep{0.1pt}
		\setlength\parskip{0.1pt}
		\item $N$: number of nodes
		\item $W$: number of workers ($N-1$)
		\item $L$: number of lines to read per file
		\item $L_w$: number of lines on the $w$th worker
		\item $M$: max number of allowed time steps
	\end{itemize}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!20]
	Indices:
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{itemize}
		\setlength\itemsep{0.1pt}
		\setlength\parskip{0.1pt}
		\item $m = 0, \cdots, M$ is the time step of the bin adaptation scheme (likely less than $M$)
		\item $n = 0, \cdots, W$ spans the nodes
		\item $w = 1, \cdots, W$ spans the workers
		\item $i = 0, \cdots, W$ spans the bin edges/indices
		\item $j = 1, \cdots, W$ spans the bin counts (this will occasionally subscript binI/E as well)
		\item $\ell_w = 0, \cdots, L_w-1$ spans the lines on the $w$th worker
		\item $k = 0, \cdots, 3$ is the data column being sorted
	\end{itemize}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!20]
	Variables:
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{itemize}
		\setlength\itemsep{0.1pt}
		\setlength\parskip{0.1pt}
		\item $\textrm{data}^w_{4\ell+k}$: data point on $\ell$th line and $k$th column (0 indexed)\\ on the $w$th worker (1 indexed)
		\item $\textrm{binE}^m_j$: bin edges (0 indexed) at time step $m$
		\item $\textrm{binI}^{w,m}_j$: bin indices on worker $w$ (0 indexed)
		\item $\textrm{binC}^{w,m}_j$: bin counts on worker $w$ (1 indexed w.r.t. $w$) at time step $m$
		\item $\textrm{binC}^{0,m}_j$: bin counts on head node (sum of worker binC's) at time step $m$
	\end{itemize}
\end{mdframed}


% JJ
\subsection{File I/O}

\input{fileio.tex}


\subsubsection{Distributing files}

\input{distributefiles.tex}

\subsubsection{Importing files}

\input{importfiles.tex}


% JF
\subsection{Sorting}

% here's how to insert code into LaTeX
\begin{lstlisting}
	walk = RandomWalk(nStep, initPos, stdDev)
\end{lstlisting}

\subsubsection{Linked list merge sort}
\subsubsection{Bubble sort}


% GW
\subsection{Binning}
% This is where the optimization stuff is
We now discuss the optimization-inspired portion of the method. It has optimization properties since we need to find the bin edges such that we maximum the uniformity of the distribution. However, it is a constrained form of optimization since the bin edges must always maintain the relation
\begin{equation}
	\textrm{binE}^m_i < \textrm{binE}^m_{i+1}
\end{equation}
As we will see, the way we construct our adaptation formulae ensures that this constraint is always met.

\subsubsection{Data binning w/ binary search}
Since we use an iterative scheme to adjust the bin edges through time, we use as an initial condition/approximation equally-spaced bin edges between the global min and max of the data. Now, since each worker could theoretically have millions of data points, repeatedly looping through the data to find the bin in which each point lies would be inefficient. As such, we use the fact that the data is sorted to expedite the process significantly by using binary search. Now, each worker will have the same bin edges $\textrm{binE}^m_i$ (where $i = 0, \cdots, W$), but the the location of the edges with respect to the data indices on each worker will likely be quite different if the data distribution across workers varies. With this in mind, for each worker, we search for the data indices at which each bin edge lies, giving us the new variables $\textrm{binI}^{w,m}_i$ (note the superscript $w$, indicating that each worker has its own unique set of bin indices). To do this, we search for the index $\ell$ such that
\begin{equation}
	\textrm{data}^w_{4\ell+k} < \textrm{binE}^m_i < \textrm{data}^w_{{4(\ell+1)+k}}
\end{equation}
giving $\textrm{binI}^{w,m}_i = \ell+1$. We also set $\textrm{binI}^{w,m}_0 = 0$ and $\textrm{binI}^{w,m}_W = L_w$. We then calculate $\textrm{binC}^{w,m}_j$:
\begin{equation}
	\textrm{binC}^{w,m}_j = \textrm{binI}^{w,m}_j - \textrm{binI}^{w,m}_{j-1}, \quad j = 1, \cdots, W
\end{equation}
Lastly, all workers send their bin counts to the head node and we calculate the total bin counts:
\begin{equation}
	\textrm{binC}^{0,m}_j = \sum_{w=1}^{W} \textrm{binC}^{w,m}_j, \quad j = 1, \cdots, W
\end{equation}


\subsubsection{Stopping criterion: the uniformity metric}
Once the head node has calculated the total bin counts, it then determines how uniformly the data distributed across the bins:
\begin{equation}
	U^m = \textrm{max} \bigg( \dfrac{\textrm{binC}^{0,m}_{\textrm{max}} - \textrm{binC}^{0,m}_{\textrm{avg}}}{\textrm{binC}^{0,m}_{\textrm{avg}}}, \dfrac{\textrm{binC}^{0,m}_{\textrm{avg}} - \textrm{binC}^{0,m}_{\textrm{min}}}{\textrm{binC}^{0,m}_{\textrm{avg}}} \bigg)
\end{equation}
If $U$ is below a set threshold (usually $\approx 0.1$), then the the data distribution is deemed to be uniform in the sense that each worker will have within $\approx 10\%$ of the average data per worker; thus sorting on each worker will take roughly equal time.


\subsubsection{Adapting bin edges}
If the data is not uniform in the first step, we move on to adapt the interior bin edges ($i = 1, \cdots, W-1$):
\begin{equation}
	\begin{split}
		\Delta C & = 2.0 ( \textrm{binC}^{0,m}_{i+1} - \textrm{binC}^{0,m}_i ) / ( \textrm{binC}^{0,m}_{i+1} + \textrm{binC}^{0,m}_i ) \\
		\Delta B & = \textrm{binE}^m_{i+1} - \textrm{binE}^m_i \\
		\textrm{binE}^{m+1}_i & = \textrm{binE}^m_i + \alpha \Delta C \Delta B
	\end{split}
\end{equation}
where $0 < \alpha < 0.5$. Each of these terms is designed the allow the bins to adapt the maximum amount possible without the bin edges becoming out of order. The quantity $\Delta B$ scales the maximum change to be within the current bin width. The quantity $0 \le \Delta C \le 1$ is a type of normalized gradient which will direct the bin edges toward regions with higher density. Lastly, $\alpha$ is a form of rate constant. It must remain less than 0.5 to maintain the constraint. It is usually set between 0.25-0.475.







% JF
\subsection{Exchanging data}
\subsubsection{Data swap}
\subsubsection{Cleanup}


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Testing}


\subsection{File I/O}

\subsection{Sorting}
\subsubsection{Linked list merge sort}
\subsubsection{Bubble sort}

\subsection{Binning}


\subsubsection{Prototyping in MATLAB}
Prior to our implementation in C++, we prototype the adaptation scheme in MATLAB as a proof of concept. For these tests, we read 1000 lines from one of the provided data files and applied the adaptive binning scheme to it, varying the number of nodes, iterations, and the value of $\alpha$.
\begin{figure}[!htb]
	\centering
	\vspace{-5pt}
	\includegraphics[scale = 0.25]{AdaptiveBinning_5Nodes_1000Lines_0475alpha}
	\vspace{-10pt}
	\caption{5 nodes, 1000 data points, $\alpha = 0.475$}
	\label{5,0.475}
\end{figure}
\begin{figure}[!htb]
	\centering
	\vspace{-5pt}
	\includegraphics[scale = 0.25]{AdaptiveBinning_10Nodes_1000Lines_0475alpha}
	\vspace{-10pt}
	\caption{10 nodes, 1000 data points, $\alpha = 0.475$}
	\label{10,0.475}
\end{figure}
\begin{figure}[!htb]
	\centering
	\vspace{-5pt}
	\includegraphics[scale = 0.25]{AdaptiveBinning_10Nodes_1000Lines_0250alpha}
	\vspace{-10pt}
	\caption{10 nodes, 1000 data points, $\alpha = 0.25$}
	\label{10,0.250}
\end{figure}

As can be seen from the Figure \ref{5,0.475}, with $W=5$ and $\alpha=0.475$, the method converges to a highly uniform distribution in roughly 20 steps. However, when increasing the number of workers to 10 (Figure \ref{10,0.475}), the method struggles to converge in the same amount of time. This problem arises because adding more workers adds more bin edges. Since the test only uses 1000 data points, then there is only an average of 100 points per bin, which is not sufficiently smooth for the method to perform at its optimal level. As such, we must decrease $\alpha$ so that we can remove the wild oscillations present in the plots. This is what Figure \ref{10,0.250} demonstrates. Though it still does not converge quite as fast as the 5 worker test, it is sufficiently better than the second test.

These tests demonstrate two general rules which describe the performance of the method. First, the method performs better (faster convergence, less oscillations) with more data since altering the bin edges on average won't have as drastic an effect on the bin counts. Also, the method performs worse (slower convergence, possible oscillatory behavior) with more workers (i.e., bin edges). There are two reasons for this. First, relating to the previous rule, since more granular divisions of the data counter the positive effects that large amounts of data have on performance. This being said, in practice, this effect is not significant when dealing with millions or billions of data points because no practical number of workers can make the data significantly granular. Second, since we use only a local bin updating scheme (a form of normalized gradient of the bin counts), the bin edges near the boundary of the data range do not have much room to move since the interior bin edges must move first (since we must maintain the constraint). Consequently, this makes our method susceptible to very poor performance if there is an outlier in the data, because there will be a large number of bin edges between the outlier and the rest of the data which cannot move until some data is propagated toward them. A clear improvement which could be made to the method would be to replace the local bin count gradient with a global analogue (whicj still maintains the constraint, of course).


\subsubsection{C++ runs}





\subsection{Exchanging data}



%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Conclusions}
We will now conclude with two discussions on 1) the most difficult and most successful aspects of our method and 2) ways of improving the both the method's performance/efficiency and our workflow as a group.

\subsection{Challenges and successes}


\subsection{Future work}



%%%%%%%%%%%%%%%
%%%%% THE END %%%%%
%%%%%%%%%%%%%%%


\end{document}





