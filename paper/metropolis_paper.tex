\documentclass{article}

\usepackage[a4paper,margin=1.15in,footskip=0.25in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textpos}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{varwidth}
\usepackage[linesnumbered,ruled]{algorithm2e}

% Theorem
\newtheorem{theorem}{Theorem}

% lists
\usepackage{outlines}
\usepackage{enumitem}
\newenvironment{tight_enum}{
\begin{enumerate}[label=\alph*.]
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
}{\end{enumerate}}

% \subsubsubsection{}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\newcommand\simpleparagraph[1]{%
  \stepcounter{paragraph}\paragraph*{\theparagraph\quad{}#1}}


%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%

\begin{document}

\title{An optimization-inspired approach to parallel sorting}
\author{Team Metropolis: \\
		James Farzi, JJ Lay, and Graham West}
\date{COMS 7900, Capstone}

\maketitle

\begin{abstract}
We present a novel method influenced by ideas in the field optimization to efficiently sort large amounts of data in parallel on a cluster of computing nodes. We describe in depth the different challenges which beset such a method, including distributing/importing files, locally sorting the data on each node, uniformly binning the data, and exchanging data between the nodes. We also present the results of several different timing tests applied to the method. These tests demonstrate how the method scales when the number of files and/or the number of nodes is increased. Finally, we summarize the the greatest challenges in implementing the method, as well as the components of the method which were the most successful. We conclude with a discussion on ways in which the method could be improved.
\end{abstract}


\tableofcontents


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{The Method}

Introduction:

We used C++ with C MPI calls

Used GitHub

Workflow description\\



Conventions (NOTE: be consistent with the indices):

$N$: number of nodes

$W$: number of workers ($N-1$)

$L$: number of lines to read per file

$L_w$: number of lines on the $w$th worker

$M$: max number of allowed time steps

Indices:
\begin{tight_enum}
	\item $m = 0, \cdots, M$ is the time step of the bin adaptation scheme (likely less than $M$)
	\item $n = 0, \cdots, W$ spans the nodes
	\item $w = 1, \cdots, W$ spans the workers
	\item $i = 0, \cdots, W$ spans the bin edges/indices
	\item $j = 1, \cdots, W$ spans the bin counts (this will occasionally subscript binI/E as well)
	\item $\ell_w = 0, \cdots, L_w-1$ spans the lines on the $w$th worker
	\item $k = 0, \cdots, 3$ is the data column being sorted
\end{tight_enum}

$\textrm{data}^w_{4\ell+k}$: data point on $k$th line and $\ell$th column (0 indexed) on the $w$th worker (1 indexed)

$\textrm{binE}_j$: bin edges (0 indexed)

$\textrm{binI}^w_j$: bin indices on worker $w$ (0 indexed)

$\textrm{binC}^w_j$: bin counts on worker $w$ (1 indexed w.r.t. $w$)

$\textrm{binC}^0_j$: bin counts on head node (sum of worker binC's)



% JJ
\subsection{File I/O}
\subsubsection{Distributing files}
\subsubsection{Importing files}


% JF
\subsection{Sorting}
\subsubsection{Linked list merge sort}
\subsubsection{Bubble sort}


% GW
\subsection{Binning}
% This is where the optimization stuff is
We now discuss the optimization-inspired portion of the method. We use an iterative scheme to adjust the bin edges through time. As an initial condition/approximation, we  set the bin edges to be equally-spaced between the global min and max of the data.

\subsubsection{Data binning w/ binary search}
Since each node could theoretically have millions of data points, repeatedly looping through the data to find the bin in which each point lies would be inefficient. As such, we use the fact that the data is sorted to expedite the process significantly by using binary search. Now, each worker will have the same bin edges $\textrm{binE}_i$ (where $i = 0, \cdots, W$), but the the location of the edges with respect to the data indices on each worker will likely be quite different if the data distribution across workers varies. With this in mind, for each worker, we search for the data indices at which each bin edge lies, giving us the new variables $\textrm{binI}^w_i$ (note the superscript $w$, indicating that each worker has its own unique set of bin indices). To do this, we search for the index $\ell$ such that
\begin{equation}
	\textrm{data}^w_{4\ell+k} < \textrm{binE}^m_i < \textrm{data}^w_{{4(\ell+1)+k}}
\end{equation}
giving $\textrm{binI}^w_i = \ell+1$. We also set $\textrm{binI}^w_0 = 0$ and $\textrm{binI}^w_W = L_w$. We then calculate $\textrm{binC}^w_j$:
\begin{equation}
	\textrm{binC}^w_j = \textrm{binI}^w_j - \textrm{binI}^w_{j-1}, \quad j = 1, \cdots, W
\end{equation}
Lastly, we calculate the total bin counts on the master node:
\begin{equation}
	\textrm{binC}^0_j = \sum_{w=1}^{W} \textrm{binC}^w_j, \quad j = 1, \cdots, W
\end{equation}


\subsubsection{Stopping criterion: the uniformity metric}


\begin{equation}
	U^n = \textrm{max}( \dfrac{c_{\textrm{max}} - c_{\textrm{avg}}}{c_{\textrm{avg}}}, \dfrac{c_{\textrm{avg}} - c_{\textrm{min}}}{c_{\textrm{avg}}} )
\end{equation}


\subsubsection{Adapting bin edges}
If the data is not uniform in the first step, we move on to adapt the bins.






% JF
\subsection{Exchanging data}
\subsubsection{Data swap}
\subsubsection{Cleanup}


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Testing}


\subsection{File I/O}

\subsection{Sorting}
\subsubsection{Linked list merge sort}
\subsubsection{Bubble sort}

\subsection{Binning}
\subsubsection{Prototyping in MATLAB}
\subsubsection{C++ runs}

\subsection{Exchanging data}



%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Conclusions}
We will now conclude with two discussions on 1) the most difficult and most successful aspects of our method and 2) ways of improving the both the method's performance/efficiency and our workflow as a group.

\subsection{Challenges and successes}


\subsection{Future work}



%%%%%%%%%%%%%%%
%%%%% THE END %%%%%
%%%%%%%%%%%%%%%


\end{document}





